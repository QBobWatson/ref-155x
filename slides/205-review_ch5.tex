
% JDR: These are the topics I thought my students found most confusing from
%   chapter 5, and the ones they asked me to cover.  This is more than 50
%   minutes worth of slides; the students can review the rest online.

\usetikzlibrary{angles,decorations.pathreplacing}

\titleframe{Review for Midterm 3}{Selected Topics}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Eigenvectors and Eigenvalues}

\vskip-3mm
\begin{defn}
  Let $A$ be an $n\times n$ matrix.
  \begin{enumerate}
  \item An \textbf{eigenvector} of $A$ is a nonzero vector $v$ in
    $\R^n$ such that $Av = \lambda v$, for some $\lambda$ in $\R$.
    In other words, $Av$ is a multiple of $v$.
  \item An \textbf{eigenvalue} of $A$ is a number $\lambda$ in $\R$ such that the
    equation $Av=\lambda v$ has a nontrivial solution.
  \end{enumerate}
  If $Av = \lambda v$ for $v\neq 0$, we say $\lambda$ is the
  \textbf{eigenvalue for $v$}, and $v$ is an \textbf{eigenvector for $\lambda$}.
\end{defn}

\pause
\begin{defn}
  Let $A$ be an $n\times n$ matrix and let $\lambda$ be an eigenvalue of $A$.
  The \textbf{$\lambda$-eigenspace} of $A$ is the set of all eigenvectors of $A$
  with eigenvalue $\lambda$, plus the zero vector:
  \[\begin{split} \text{$\lambda$-eigenspace}
    &= \bigl\{ v\text{ in }\R^n\mid Av = \lambda v \bigr\} \\
    &= \bigl\{ v\text{ in }\R^n\mid (A-\lambda I)v = 0 \bigr\} \\
    &= \Nul\bigl( A-\lambda I \bigr).
  \end{split}\]
\end{defn}

\pause
You find a basis for the $\lambda$-eigenspace by finding the parametric vector
form for the general solution to $(A-\lambda I)x=0$ using row reduction.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{The Characteristic Polynomial}

\vskip-3mm
\begin{defn}
  Let $A$ be an $n\times n$ matrix.  The \textbf{characteristic polynomial} of
  $A$ is 
  \[ f(\lambda) = \det(A-\lambda I). \]
\end{defn}

\pause
\alert{Important Facts:}
\pause
\begin{enumerate}
\item The characteristic polynomial is a polynomial of degree $n$, of the
  following form:
  \[ f(\lambda)
  = (-1)^n\lambda^n + a_{n-1}\lambda^{n-1} + a_{n-2}\lambda^{n-2} 
  + \cdots + a_1\lambda + a_0. \]
  \vskip-3mm
  \pause
\item The eigenvalues of $A$ are the roots of $f(\lambda)$.
  \pause
\item The constant term $f(0) = a_0$ is equal to $\det(A)$:
  \[ f(0) = \det(A-0I) = \det(A). \]
  \vskip-5mm\pause
\item The characteristic polynomial of a $2\times 2$ matrix $A$ is
  \[ f(\lambda) = \lambda^2 - \Tr(A)\,\lambda + \det(A). \]
\end{enumerate}

\pause
\begin{defn}
  The \textbf{algebraic multiplicity} of an eigenvalue $\lambda$ is its
  multiplicity as a root of the characteristic polynomial.
\end{defn}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Similarity}

\vskip-3mm
\begin{defn}
  Two $n\times n$ matrices $A$ and $B$ are \textbf{similar} if there is an
  invertible $n\times n$ matrix $P$ such that
  \[ A = PBP\inv. \]
\end{defn}

\pause
\alert{Important Facts:}
\pause
\begin{enumerate}
\item Similar matrices have the same characteristic polynomial.
\pause
\item It follows that similar matrices have the same eigenvalues.
\pause
\item If $A$ is similar to $B$ and $B$ is similar to $C$, then $A$ is similar to
  $C$. 
\end{enumerate}

\pause\medskip
\alert{Caveats:}
\pause
\begin{enumerate}
\item Matrices with the same characteristic polynomial need not be similar.
\pause
\item Similarity has nothing to do with row equivalence.
\pause
\item Similar matrices usually do not have the same eigenvectors.
\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Similarity}
\framesubtitle{Geometric meaning}

Let $A = PBP\inv$, and let $v_1,v_2,\ldots,v_n$ be the columns of $P$.
\pause
These form a basis $\cB$ for $\R^n$ because $P$ is invertible.
\pause
\emph{Key relation:} for any vector $x$ in $\R^n$,
\[ [Ax]_\cB = B[x]_\cB. \]
\pause
This says:
\begin{center}
  $A$ acts on the usual coordinates of $x$\\
  in the same way that\\
  $B$ acts on the $\cB$-coordinates of $x$.
\end{center}

\pause\smallskip
\alert{Example:}
\vskip-2mm
\[ A = \frac 14\mat{5 3; 3 5} \qquad
B = \mat{2 0; 0 1/2} \qquad
P = \mat{1 1; 1 -1}. \]
\pause
Then $A=PBP\inv$.
\pause
$B$ acts on the usual coordinates by
\pause
scaling the first coordinate by $2$, and the second by $1/2$:
\displayskips{3pt}
\[ B\vec{x_1 x_2} = \vec{2x_1, x_2/2}. \]
\pause
The unit coordinate vectors are eigenvectors: $e_1$ has eigenvalue
\pause
$2$, and $e_2$ has eigenvalue
\pause
$1/2$.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Similarity}
\framesubtitle{Example}

\vskip-5mm
\[\hss A = \frac 14\mat{5 3; 3 5} \qquad
B = \mat{2 0; 0 1/2} \qquad
P = \mat{1 1; 1 -1} \qquad
[Ax]_\cB = B[x]_\cB.\hss \]
In this case, $\cB = \bigl\{{1 \choose 1}, {1 \choose -1}\bigr\}$.
Let $v_1 = {1\choose 1}$ and $v_2 = {1\choose -1}$.

\pause\medskip
\begin{columns}[onlytextwidth]
\column{.5\textwidth}
To compute $y = Ax$: $\phantom{1\choose 1}$
\begin{enumerate}
\item<3-> Find $[x]_\cB$. $\phantom{1\choose 1}$
\item<5-> $[y]_\cB = B[x]_\cB$. $\phantom{1\choose 1}$
\item<7-> Compute $y$ from $[y]_\cB$. $\phantom{1\choose 1}$
\end{enumerate}
\column{.5\textwidth}
Say $x = {2\choose 0}$.
\begin{enumerate}
\item<4-> $x = v_1+v_2$ so $[x]_\cB = {1\choose 1}$.
\item<6-> $[y]_\cB = B{1\choose 1} = {2\choose 1/2}$.
\item<8-> $y = 2v_1 + \frac 12v_2 = {5/2 \choose 3/2}$.
\end{enumerate}
\end{columns}

\pause[9]\smallskip\alert{Picture:}
\begin{center}
\begin{tikzpicture}[scale=.5, thin border nodes]
  \draw[very thin, black!20] (-3,-3) grid (3,3);
  \draw[->] (-3,0) -- (3,0);
  \draw[->] (0,-3) -- (0,3);
  \point (o) at (0,0);
  \draw[vector] (0,0) -- (1,1) node[above] {$v_1$};
  \draw[vector] (0,0) -- (1,-1) node[below] {$v_2$};
  \point[seq-red, "$x$" text=seq-red] (x) at (2,0);
  \draw[densely dashed, very thin] (1,1) -- (x) (1,-1) -- (x);
  \coordinate (left) at (3,0);

  \begin{scope}[xshift=12cm]
  \draw[very thin, black!20] (-3,-3) grid (3,3);
  \draw[->] (-3,0) -- (3,0);
  \draw[->] (0,-3) -- (0,3);
  \point (o) at (0,0);
  \draw[vector] (0,0) -- (2,2) node[above] {$Av_1$};
  \draw[vector] (0,0) -- (.5,-.5) node[below] {$Av_2$};
  \point[seq-red, "$Ax$" {below,text=seq-red}] (Ax) at (2.5,1.5);
  \draw[densely dashed, very thin] (2,2) -- (Ax) (.5,-.5) -- (Ax);
  \coordinate (right) at (-3,0);
  \end{scope}

  \draw[->, shorten=3mm] (left) to[bend left]
    node[above=1mm] {$A$}
    node[below=4mm, align=center, text width=2.8cm]
      {$A$ scales the $v_1$-coordinate by $2$, and
       the $v_2$-coordinate by $\frac 12$.}
    (right);

\end{tikzpicture}
\end{center}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Diagonalization}

\vskip-3mm
\displayskips{2pt}
\begin{defn}
  An $n\times n$ matrix $A$ is \textbf{diagonalizable} if it is similar to a
  diagonal matrix:
  \[ A = PDP\inv \sptxt{for $D$ diagonal.} \]
\end{defn}

\pause
It is easy to take powers of diagonalizable matrices:
\[ A^n = PD^n P\inv. \]

\pause
\begin{oneoffthm}{The Diagonalization Theorem}
  An $n\times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$
  linearly independent eigenvectors.
  In this case, $A = PDP\inv$ for
  \[ P = \mat{| |,, |; v_1 v_2 \cdots, v_n; | |,, |}
  \qquad
  D = \mat{\lambda_1 0 \cdots, 0; 0 \lambda_2 \cdots, 0;
    \vdots, \vdots, \ddots, \vdots; 0 0 \cdots, \lambda_n},
  \]
  where $v_1,v_2,\ldots,v_n$ are linearly independent eigenvectors,
  and $\lambda_1,\lambda_2,\ldots,\lambda_n$ are the corresponding eigenvalues
  (in the same order).
\end{oneoffthm}

\pause
\begin{cor}
  An $n\times n$ matrix with $n$ distinct eigenvalues is diagonalizable.
\end{cor}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Non-Distinct Eigenvalues}

\vskip-3mm
\begin{defn}
  Let $A$ be a square matrix with eigenvalue $\lambda$.  The
  \textbf{geometric multiplicity} of $\lambda$ is the dimension of the
  $\lambda$-eigenspace.
\end{defn}

\pause
\begin{thm}
  Let $A$ be an $n\times n$ matrix.  Then $A$ is diagonalizable if and only if,
  for every eigenvalue $\lambda$, the algebraic multiplicity of $\lambda$ is
  equal to the geometric multiplicity.\\
  \pause\smallskip
  (And all eigenvalues are real, unless you want to diagonalize over $\C$.)
\end{thm}

\pause\medskip
\alert{Notes:}
\begin{itemize}
\item The algebraic and geometric multiplicities are both whole numbers $\geq 1$,
  and the algebraic multiplicity is always greater than or equal to the
  geometric multiplicity.
  \pause
  In particular, they're equal if the algebraic multiplicity is $1$.
\pause
\item Equivalently, $A$ is diagonalizable if and only if the sum of the
  geometric multiplicities of its eigenvalues is $n$.

\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Non-Distinct Eigenvalues}
\framesubtitle{Example}

\vskip-3mm
\[ A = \mat{1 1 0; 0 1 0; 0 0 2} \]

This has eigenvalues
\pause
$1$ and $2$,
with algebraic multiplicities
\pause
$2$ and $1$, respectively.

\pause\medskip
The geometric multiplicity of $2$ is
\pause
\emph{automatically $1$}.

\pause\medskip
Let's compute the geometric multiplicity of $1$:
\[ A - I = \mat{0 1 0; 0 0 0; 0 0 1}
\;\longsquiggly[rref]\; \mat{0 1 0; 0 0 1; 0 0 0}. \]
\pause
This has \blankuntil{8}{$1$} free variable, so the geometric multiplicity of $1$
is \blankuntil{8}{$1$}.
\pause[9]
This is less than the algebraic multiplicity, so the matrix is
\emph{not diagonalizable}.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Stochastic Matrices}

\vskip-3mm
\begin{defn}
  A square matrix $A$ is \textbf{stochastic} if all of its entries are
  nonnegative, and the sum of the entries of each column is $1$.
  It $A$ is \textbf{positive} if all of its entries are positive.
\end{defn}

\pause
\begin{defn}
  A \emph{steady state} for a stochastic matrix $A$ is an eigenvector $w$ with
  eigenvalue $1$, such that its entries are positive and sum to $1$.
\end{defn}

\pause
\begin{oneoffthm}{Perron--Frobenius Theorem}
  If $A$ is a positive stochastic matrix, then it admits a unique steady state
  vector $w$, which spans the $1$-eigenspace.

  \smallskip
  Moreover, for any vector $v_0$ with entries summing to some number
  $c$, the iterates $v_1 = Av_0$, $v_2 = Av_1$, \ldots, $v_n = Av_{n-1}$,
  \ldots, approach $cw$ as $n$ gets large.
\end{oneoffthm}

\pause\medskip\small
Think about it in terms of Red Box movies:
\pause
$v_n$ is the number of movies in each location on day $n$, and $v_{n+1}=Av_n$.
\pause
Eventually, the number of movies in each location will be the same every day:
$v_n = v_{n+1} = Av_n$.
\pause
This means $v_n$ is an eigenvector with eigenvalue $1$, so it is a multiple of
the steady state $w$: $v_n = cw$.
\pause
The steady state $w$ tells you the \emph{percentages} of movies that are in each
location, so $c$ is the total number of movies.
\pause
So if you started with $c=100$ movies on day $0$, then you know
$v_n = cw = 100w$ for large enough $n$: the total number of movies doesn't
change.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Computing the Steady State}

\vskip-3mm
\[ A = \mat{.3 .4 .5; .3 .4 .3; .4 .2 .2} \]
This is a positive stochastic matrix.
\pause
To compute the steady state, first we find \emph{some} eigenvector with
eigenvalue $1$:
\[ A - I = \mat{-.7 .4 .5; .3 -.6 .3; .4 .2 -.8}
\;\longsquiggly[rref]\;
\mat{1 0 -7/5; 0 1 -6/5; 0 0 0}.
\]
\pause
The parametric vector form is $\vec{x y z} = z\vec{7/5 6/5 1}$, so an
eigenvector is $\vec{7 6 5}$.
\pause\\[1mm]
We want the entries of our eigenvector to sum to $1$, so we need to divide by
the sum of the entries:
\[ w = \pause \frac 1{7+6+5}\vec{7 6 5} = \frac 1{18}\vec{7 6 5}. \]
\pause
This is the steady state.
\pause
If $v = (6,22,8)$ then $A^nv$ approaches
\pause
\rlap{$36w = (14,12,10)$.}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Complex Eigenvectors}

Complex eigenvalues and eigenvectors work just like their real counterparts,
with the additional fact:
\pause\smallskip
\begin{bluebox}{.6\linewidth}
  Both eigenvalues and eigenvectors of real square matrices occur in conjugate pairs.
\end{bluebox}

\pause\medskip\displayskips{3pt}%
\alert{Example:} $A = \mat{\sqrt 3+1 -2; 1 \sqrt 3-1}$.
\pause
The characteristic polynomial is
\begin{webonly}
\[ f(\lambda) = \lambda^2 - \Tr(A)\,\lambda + \det(A)
  = \lambda^2 - 2\sqrt 3\,\lambda + 4.
\]
\end{webonly}
The quadratic formula tells us the eigenvalues are
\begin{webonly}
\[ \lambda = \frac{2\sqrt 3\pm\sqrt{(2\sqrt 3)^2-16}}2
= \sqrt 3\pm i. \]
\end{webonly}
Let's compute an eigenvector $v$ with eigenvalue $\lambda=\sqrt 3-i$.
\begin{webonly}
\[ A-\lambda I = \mat{1+i -2; \star, \star} 
\;\longsquiggly\; v = \vec{2 1+i}. \]
\end{webonly}%
\pause
An eigenvector with eigenvalue $\sqrt 3 + i$ is (automatically)
\pause
${2\choose 1-i}$.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Geometric Interpretation of Complex Eigenvalues}

\vskip-3mm
\begin{thm}
  Let $A$ be a $2\times 2$ matrix with complex (non-real) eigenvalue $\lambda$,
  and let $v$ be an eigenvector.  Then
  \[ A = PCP\inv \]
  where
  \[ P = \mat{| |; \Re v \Im v; | |}
  \sptxt{and}
  C = \mat[r]{\Re\lambda, \Im\lambda; -\Im\lambda, \Re\lambda}. \]

  \smallskip
  The matrix $C$ is a composition of a counterclockwise rotation by 
  $-\arg(\lambda)$, and a scale by a factor of $|\lambda|$.
\end{thm}

\pause\smallskip
\alert{Example:}\vskip-2mm
\displayskips{2pt}
\[ A = \mat{\sqrt 3+1 -2; 1 \sqrt 3-1} 
\qquad
\lambda = \sqrt 3-i
\qquad
v = \vec{1-i 1}
\]
\pause
This gives
\[\begin{split}
  C &= \webonlycmd{\mat[r]{\Re\lambda, \Im\lambda; -\Im\lambda, \Re\lambda}
  = \mat{\sqrt 3 -1; 1 \sqrt 3}} \\
  P &= \webonlycmd{\mat[l]{\Re(1-i) \Im(1-i); \Re(1) \Im(1)} = \mat{1 -1; 1 0}} 
\end{split}\]

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Geometric Interpretation of Complex Eigenvalues}
\framesubtitle{Example}

\vskip-6mm
\[ A = \mat{\sqrt 3+1 -2; 1 \sqrt 3-1} \quad 
C = \mat{\sqrt 3 -1; 1 \sqrt 3} \quad
P = \mat{1 -1; 1 0} \quad
\lambda = \sqrt 3-i \]
\pause
The Theorem says that $C$ scales by a factor of
\[ |\lambda| = \sqrt{(\sqrt 3)^2 + (-1)^2} = \sqrt{3+1} = 2. \]
\pause
It rotates counterclockwise by the argument of $\bar\lambda = \sqrt 3+i$,
which is $\pi/6$:\\[1mm]
\hfill
\begin{tikzpicture}[thin border nodes, decoration={brace,raise=2pt}]
  \coordinate (l) at ({sqrt(3)},1);
  \coordinate (o) at (0,0);
  \coordinate (x) at (1,0);
  \draw (-1,0) -- (2,0);
  \draw (0,-.5) -- (0,1.2);
  \draw[vector] (0,0) -- node[auto] {$\bar\lambda$} (l);
  \pic[draw, "$\theta$", angle radius=.75cm, angle eccentricity=.75]
    {angle = x--o--l};
  \draw[decoration=mirror, decorate]
    (0,0) -- node[below=5pt] {$\sqrt 3$} ({sqrt(3)},0);
  \draw[decoration=mirror, decorate]
    ({sqrt(3)},0) -- node[right=5pt] {$1$} ++(0,1);
  \point at (0,0);

  \node[font=\normalsize] at (5,1/2)
    {$\displaystyle\theta = \tan\inv\left( \frac 1{\sqrt 3} \right) = \frac\pi 6$};
\end{tikzpicture}\hfill\null\\
\pause\hfill\def\theo{\includegraphics[width=4cm]{figures/theo5.jpg}}%
\begin{tikzpicture}
  \useasboundingbox (3.5,{sqrt(2)}) -- (3.5,{-sqrt(2)});
  \begin{scope}
    \clip[cm={1,1,-1,0,(0,0)}] (0,0) ellipse[radius=1/sqrt(2)];
    \node[scale=1/2] (theo1) at (0,0) {\theo};
  \end{scope}

  \begin{scope}[opacity=.6, black!15]
    \draw[->] (-1.3,0) -- (1.3,0) node[coordinate] (l) {};
    \draw[->] (0,-1.3) -- (0,1.3);
  \end{scope}

  \begin{scope}[cm={1,1,-1,0,(0,0)}]
    \draw[opacity=.6, black!15] (0,0) circle[radius=1];
    \draw[->,red] (1.1,0) arc[radius=1.1,start angle=0,delta angle=30];
  \end{scope}
  \draw[vector] (0,0) -- (1,1);
  \draw[vector] (0,0) -- (-1,0);

  \begin{scope}[xshift=7cm]
    \begin{scope}[cm={(sqrt(3)+1)/2,1/2,-1,(sqrt(3)-1)/2,(0,0)}]
      \clip[cm={1,1,-1,0,(0,0)}] (0,0) ellipse[radius=sqrt(2)];
      \node[transform shape] (theo2) at (0,0) {\theo};
    \end{scope}
    \begin{scope}[opacity=.6, black!15]
      \draw[->] (-1.3,0) node[coordinate] (r) {} -- (1.3,0);
      \draw[->] (0,-1.3) -- (0,1.3);
      \draw[cm={1,1,-1,0,(0,0)}] (0,0) ellipse[radius=1];
    \end{scope}
  \end{scope}

  \draw[->] ($(l) + (3mm,3mm)$) to[bend left]
    node[midway,above] {$A$}
    node[midway,below=.5cm,align=center] 
      {``rotate around an ellipse''\\
      scale by $2$}
    ($(r) + (-3mm,3mm)$);

\end{tikzpicture}
\hfill\null

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Computing the Argument of a Complex Number}
\framesubtitle{Caveat}

\alert{Warning:} if $\lambda = a+bi$, you can't just plug $\tan\inv(b/a)$ into
your calculator and expect to get the argument of $\lambda$.

\pause\medskip
\alert{Example:} If $\lambda = -1-\sqrt 3i$ then
\[ \tan\inv\left( \frac{-\sqrt 3}{-1} \right) 
= \tan\inv(\sqrt 3) = \pause \frac\pi 3. \]
\pause
Anyway that's the number your calculator will give you.

\pause\medskip
You have to \emph{draw a picture}:
\begin{center}
\begin{tikzpicture}[thin border nodes, decoration={brace,raise=2pt}]
  \coordinate (l) at (-1,{-sqrt(3)});
  \coordinate (o) at (0,0);
  \coordinate (x) at (-1,0);
  \draw (-1.5,0) -- (.5,0);
  \draw (0,-2) -- (0,.5);
  \draw[vector] (0,0) -- node[auto] {$\lambda$} (l);
  \pic[draw, "$\theta$"]
    {angle = x--o--l};
  \draw[decoration=mirror, decorate]
    (0,0) -- node[above=5pt] {$1$} (-1,0);
  \draw[decoration=mirror, decorate]
    (-1,0) -- node[left=5pt] {$\sqrt 3$} (-1,{-sqrt(3)});
  \point at (0,0);

  \node[align=left, font=\normalsize] at (3,-1/2)
    {$\displaystyle\theta = \tan\inv(\sqrt 3) = \frac\pi 3$\\
    argument = $\displaystyle\theta+\pi = \frac{4\pi}3$};
\end{tikzpicture}
\end{center}
\note{No calculators on exams.}


\end{frame}




%%% Local Variables:
%%% TeX-master: "../slides"
%%% End:
